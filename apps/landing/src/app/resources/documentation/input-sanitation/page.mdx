---
title: Pipe0 Docs | Input Sanitation
description: "Pipe0 takes care of cleaning up your input data by default"
date: 2025-03-07
---

# Input Sanitation

Data enrichment systems ingest user-provided data from sources like:

- CRMs
- ATSs
- Web forms

User-provided data can contain mistakes or be invalid. Pipe0 has a robust sanitation layer to clean
and regenerate data.

## Cleanup

The following <AppLink linkType='pipelineRequest'>request payload</AppLink> contains
common errors but will be processed successfully.

```json
{
  "pipes": [
    { 
      "pipe_id": "company:identity@1",
    },
  ],
  "input": [
    {
      "id": 1,
      "name": "Susi Jui",
      "company_websiste_url": "pipe0.com", // missing protocol 'https://'
      "email": "mailto:susi@pipe0.com", // "malto:" prefix
      "personal_website_url": "wwwww.susi.com" // wwww instad of www
    },
    {
      "id": 2,
      "name": "Tom Schmidt",
      "company_name": "Pipe0",
      "company_websiste_url": "not today" // invalid: expected URL
    },
  ],
}
```

Here's how we clean this request:

1. Parse URLs into a consistent format and clean common mistakes
1. Parse email addresses into a consistent format and clean common mistakes
1. Fix obvious typos and remove invalid characters
1. Parse data formats on demand (int > float, float > int, int > string)

## Regeneration

In our example, `"company_websiste_url": "not today"` is not a valid URL.

Because `company_websiste_url` is an <AppLink linkType="outputField">output field</AppLink> of `company:identity@1`,
we can find the correct value.

During processing, `company:identity@1` detects that `company_websiste_url` is of invalid format 
and replaces it with the correct value.

The result may look like this:

```json
{
    "id": 2,
    "name": "Tom Schmidt",
    "company_name": "Pipe0",
    "company_websiste_url": "https://valid-url.com" // healed
},
```

> [!NOTE]
> 
> Valid input values are not regenerated. Instead, they are copied from the `input` to the <AppLink linkType="record">record</AppLink>.

## Incomplete data

It is common for input data to be incomplete.

Failing the entire task because one input object cannot be processed is impractical and annoying.

### Partially missing input fields

If we find **at least one input object** that can be processed, pipeline validation will pass.

Let's look at the following request payload:

```json
{
  "pipes": [
    { 
      "pipe_id": "company:identity@1",
    },
  ],
  "input": [
    {
      "id": 1,
      "name": "Susi Jui",
      "company_name": "Pipe0",
      "company_websiste_url": "pipe0.com",
    },
    { // CANNOT be processed by "company:identity@1"
      "id": 2,
      // required `company_name` missing
    },
  ],
}
```

The pipe `company:identity@1` requires the input field `company_website_url` which is not present
in record `id=2`. In this case:

- Pipeline validation passes
- Record `id=1` is processed in full.
- Record `id=2` has failed fields

### No input object has required input fields

Let's look at another example:


```json
{
  "pipes": [
    { 
      "pipe_id": "company:identity@1",
    },
  ],
  "input": [
    { // CANNOT be processed by "company:identity@1"
      "id": 1,
      "name": "Susi Jui",
    },
    { // CANNOT be processed by "company:identity@1"
      "id": 2,
      "name": "Tom Schmidt"
    },
  ],
}
```

No input object has the required field `company_websiste_url`. The request will fail during pipeline validation.

The entire task will fail before processing starts.

### Never fail a task

In practice, dealing with failing tasks can be annoying. If you don't want to deal 
with failing task, there's an escape hatch: If you define the expected input fields and set them to `null`, pipeline validation will
pass. The task will not fail. Instead, only individual fields fail.

